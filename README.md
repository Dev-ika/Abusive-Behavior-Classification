# Abusive-Behavior-Classification
Multi-headed classification models for identifying abusive levels in text and their comparative study


An application to monitor the use of abusive and vulgar language on chats on social
media networks to prevent users from being obscene can play a part in modifying the
way users interact with each other. “Friendly chat” is a play on a group chatting
application, users can discuss a range of topics that is completely unrestricted. When
things start heating up a user may become abusive towards other users, at times like this
the application will quickly diffuse the situation by deploying preventive measures,
stopping the abusive user in his tracks.
Abuse however is not the only form of “toxic” content a user will be exposed to while
interacting with other users. Identity hate, threats and other forms of obscenity may also
become a part of the conversation. To classify a message as toxic we need a baseline to
refer to, we use the “toxic comment” dataset which consists of a large number of
Wikipedia comments which have been rated by human rater’s who classify the comment
to fall under one or more of the above-mentioned categories. Integrate this model into the
real time data we obtain from “Friendly chat” to classify a user into one or more of these
categories. Using classification and machine learning techniques such as simple Naive
Bayes along with linear regression and LSTM models
our application able to detect when a user is abusive.
